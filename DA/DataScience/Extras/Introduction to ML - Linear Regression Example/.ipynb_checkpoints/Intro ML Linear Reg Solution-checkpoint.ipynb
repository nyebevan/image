{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to ML - M. Linear Regression Example for Beginners"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. What is Machine Learning (ML) in a nutshell\n",
    "\n",
    "- “Machine learning is the science (and art) of programming computers so they can learn from data” by Aurélien Géron book (Hands-On Machine Learning with Scikit-Learn and TensorFlow) \n",
    "\n",
    "\n",
    "- ML uses statistical models and algorithms to perform tasks like predictions & classifications without explicit instructions \n",
    "\n",
    "\n",
    "- ML is a subset of Artificial Intelligence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Machine Learning Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Graphical ML Process\n",
    "\n",
    "import os\n",
    "from IPython.display import Image\n",
    "PATH = \"..\\\\Introduction to ML - Logistic Regression Example\\\\\"\n",
    "Image(filename = PATH + \"Machine Learning.png\", width=900, height=900)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial Overview\n",
    "\n",
    "##### Video 1:\n",
    "1. What is Machine Learning\n",
    "2. Process of Machine Learning\n",
    "3. Problem Formulation\n",
    "\n",
    "##### Video 2:\n",
    "4. Loading the Raw Data\n",
    "5. Data Preprocessing\n",
    "    - EDA\n",
    "    - Data Cleaning\n",
    "    - Feature Selection\n",
    "6. Splitting the Raw Data\n",
    "\n",
    "##### Video 3:\n",
    "7. What is Linear Regression Analysis\n",
    "8. Running Regression\n",
    "9. Evaluating the Model\n",
    "10. How to use our L. Regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Problem Formulation\n",
    "\n",
    "- In this example, we want to investigate \"what factors/variables affect the performance of revenue\"\n",
    "- Our aim is to go back to the business and make suggestions about how to generate more revenue\n",
    "- Make Revenue predictions given specific variable values\n",
    "- Hence our Dependent variable (y) is Revenue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing / Installing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages / libraries\n",
    "import os #provides functions for interacting with the operating system\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, explained_variance_score, mean_absolute_error, mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# To install sklearn type \"pip install numpy scipy scikit-learn\" to the anaconda terminal\n",
    "\n",
    "# To change scientific numbers to float\n",
    "np.set_printoptions(formatter={'float_kind':'{:f}'.format})\n",
    "\n",
    "# Increases the size of sns plots\n",
    "sns.set(rc={'figure.figsize':(12,10)})\n",
    "\n",
    "# import sys\n",
    "# !conda list Check the packages installed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Loading the Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data\n",
    "raw_data = pd.read_csv('..\\\\Introduction to ML - Linear Regression Example\\\\Marketing Raw Data.csv')\n",
    "\n",
    "# print the shape\n",
    "print(raw_data.shape)\n",
    "\n",
    "#runs the first 5 rows\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for null values\n",
    "\n",
    "raw_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the NULL observations\n",
    "\n",
    "\n",
    "raw_data[raw_data['Week'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ways of dealing with missing data\n",
    "- Delete the whole observation\n",
    "- Replace the NULL value with another value (mean, rolling mean, last day value, closest day value, mode, median, zero, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting the NULL values\n",
    "raw_data = raw_data.dropna(subset = ['Week'])\n",
    "\n",
    "# Printing the shape\n",
    "raw_data.shape\n",
    "\n",
    "# Visualize the NULL observations\n",
    "raw_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate all the elements whithin each Feature \n",
    "\n",
    "for column in raw_data:\n",
    "    unique_vals = np.unique(raw_data[column])\n",
    "    nr_values = len(unique_vals)\n",
    "    if nr_values < 10:\n",
    "        print('The number of values for feature {} :{} -- {}'.format(column, nr_values,unique_vals))\n",
    "    else:\n",
    "        print('The number of values for feature {} :{}'.format(column, nr_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the data using seaborn Pairplots\n",
    "\n",
    "g = sns.pairplot(raw_data)\n",
    "\n",
    "# Notes: Do not run this on a big dataset. Filter the columns first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualising a Subset of our data - important features\n",
    "\n",
    "g = sns.pairplot(raw_data[['Visitors', 'Revenue', 'Marketing Spend', 'Promo']], hue = 'Promo', height = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualising a Subset of our data - important features\n",
    "\n",
    "g = sns.pairplot(raw_data[['Visitors', 'Revenue', 'Marketing Spend', 'Day_Name']], hue = 'Day_Name', height = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualising a Subset of our data - important features\n",
    "\n",
    "g = sns.lmplot(x = 'Marketing Spend', y = 'Revenue', data = raw_data, col = 'Day_Name', col_wrap = 3, height = 5, \n",
    "              scatter_kws = {'color':'green'}, ci = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate the distribution of Revenue by Categorical Variables\n",
    "\n",
    "pal = ['green', 'blue','red']\n",
    "\n",
    "g = sns.boxplot(x = 'Day_Name', y = 'Revenue', data = raw_data, hue = 'Promo', palette = pal)\n",
    "               \n",
    "#ax = sns.swarmplot(x = 'Day_Name', y = 'Revenue', data = raw_data, palette = pal, hue = 'Promo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting outliers\n",
    "\n",
    "raw_data = raw_data[raw_data['Revenue'] < 27000]\n",
    "\n",
    "raw_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column names\n",
    "raw_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting usefull columns only\n",
    "raw_data = raw_data[['Day_Name','Visitors', 'Revenue', 'Marketing Spend', 'Promo']]\n",
    "\n",
    "#visualize the raw data\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making categorical variables into numeric representation\n",
    "\n",
    "new_raw_data = pd.get_dummies(raw_data, columns = ['Promo', 'Day_Name'])\n",
    "\n",
    "# Notes:\n",
    "# We can also do this with Label Encoding and OneHotEncoder from the preprocessing library\n",
    "\n",
    "# Visualizing the data\n",
    "new_raw_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. Feature Selection\n",
    "\n",
    "In this example, we do not have many variables so we should use all of the data but in some cases, you have thousands of variables and you will need to filter them in order to save computational time\n",
    "\n",
    "- 2 ways to help us select the important features are:\n",
    "    - Correlation \n",
    "    - Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running Correlation\n",
    "\n",
    "hm = new_raw_data[['Visitors','Revenue','Marketing Spend']].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes\n",
    "- .corr() is used to find the pairwise correlation of all columns in the dataframe. Any null values are automatically excluded\n",
    "- The closer to 1 or -1 the better. As one variable increases, the other variable tends to also increase / decrease\n",
    "- 0.8 +- is Strong Correlation, 0.6 to 0.8 +- is moderate Correlation & the other values, there is no correlation\n",
    "- More Info here: https://statisticsbyjim.com/basics/correlations/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing Correlation with a Heatmap\n",
    "\n",
    "g = sns.heatmap(hm, annot = True, annot_kws={'size':50})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Steps of Running Feature Importance\n",
    "- Split the data into X & y\n",
    "- Run a Tree-based estimators (i.e. decision trees & random forests) \n",
    "- Run Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into X & y\n",
    "\n",
    "X = new_raw_data.drop('Revenue', axis = 1).values\n",
    "X2 = new_raw_data.drop('Revenue', axis = 1)\n",
    "y = new_raw_data['Revenue']\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a Tree-based estimators (i.e. decision trees & random forests)\n",
    "\n",
    "dt = DecisionTreeClassifier(random_state=15, criterion  = 'entropy', max_depth = 10)\n",
    "dt.fit(X,y)\n",
    "\n",
    "# If you want to learn how Decesion Trees work, read here: https://www.datacamp.com/community/tutorials/decision-tree-classification-python\n",
    "# Official Doc: https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes:\n",
    "- The importance of a feature is calculated as the (normalized) total reduction of entropy (other criterions too) brought by that feature or the higher information gain\n",
    "- To understand the maths, read this: https://towardsdatascience.com/the-mathematics-of-decision-trees-random-forest-and-feature-importance-in-scikit-learn-and-spark-f2861df67e3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running Feature Importance\n",
    "\n",
    "for i, column in enumerate(new_raw_data.drop('Revenue', axis = 1)):\n",
    "    print('The feature importance for {} is: {:.3f}'.format(column, dt.feature_importances_[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes:\n",
    "- Please note that we have not normalised / scale our data\n",
    "- Please note that we have not done any feature engineering - created new features\n",
    "- Please note that we have not joined multiple datasets together\n",
    "- Please note that we have not aggregated any of our data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Splitting the Raw Data - Hold-out validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hold-out validation\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.80, test_size = 0.2, random_state=15)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "# Official Doc: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. What is Linear Regression Analysis\n",
    "\n",
    "- Linear regression is a widely used machine learning model that predicts continues numbers as outputs; not classes \n",
    "- It models the relationship of 2 plus variables by fitting a linear equation to observed data \n",
    "- In linear regression you have 1 dependent variable (y) and 1 plus independent variables (X)\n",
    "- Before you attempt to model the data, you should check if there is a relationship between the variables first; a good way is to use a scatterplot to visualise the data\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Graphical Anscombe's_quartet\n",
    "\n",
    "PATH = \"..\\\\Introduction to ML - Linear Regression Example\\\\\"\n",
    "Image(filename = PATH + \"Anscombe's_quartet.png\", width=900, height=900)\n",
    "\n",
    "\n",
    "# identical descriptive statistics\n",
    "# identical mean, std, variances, correlations, and regression lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Linear Regression Analysis (continued)\n",
    "\n",
    "- A measurement of the relationship between 2 variables is the correlation coefficient; ranging from -1 to 1; the closer to 1 or -1, the stronger the relationship\n",
    "\n",
    "- However, even if there seems to be a relationship, you should always be careful when modelling the relationship between 2 variables as correlation is not causation. For example, the rise of temperature vs Revenue\n",
    "\n",
    "- The mathematical equation of linear regression is Y = a + bX; where X is the independent variable and Y is the dependent variable. ‘b’ is the slope of the line and 'a' is the intercept; the value of y when x = 0\n",
    "\n",
    "\n",
    "- The most common cost function used in linear regression is the “Least Squared Errors” function; which is the sum of squared errors (sum(y actuals – y predicted) ^ 2) over the training set; trying to minimize how far off the predictions are from the actuals.\n",
    "\n",
    "- To calculate the “Least Squared Errors” firstly you must calculate the difference of the actual (y) vs the p(y); this is called the residuals.\n",
    "\n",
    "- In order to find the least squared error you will have to find the optimal parameter values (b) that minimize the sum ‘S’ of squared residuals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How L Regression iterates until convergence\n",
    "\n",
    "PATH = \"..\\\\Introduction to ML - Linear Regression Example\\\\\"\n",
    "Image(filename = PATH + \"Regression Example.png\", width=900, height=900)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is Linear Regression Analysis (continued)\n",
    "\n",
    "- The optimisation technique used in linear regression is Gradient Descent; which attempts to find a local or a global minimum of a cost function\n",
    "\n",
    "- Gradient descent finds the direction ‘gradient’ that the model/line should take so that the errors will be reduced. \n",
    "\n",
    "- Direction refers to the weights (a and b), and how those weights should be tweaked to further reduce the errors. \n",
    "\n",
    "- The model will iterate until converged; no further improvements can be done; see graph below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Descent\n",
    "\n",
    "PATH = \"..\\\\Introduction to ML - Linear Regression Example\\\\\"\n",
    "Image(filename = PATH + \"Gradient Descent.png\", width=900, height=900)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Running Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the Regression\n",
    "\n",
    "lm = LinearRegression(fit_intercept = True)\n",
    "lm.fit(X_train, y_train)\n",
    "\n",
    "y_pred = lm.predict(X_train)\n",
    "# SK-Learn official doc: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Accuracy on training dataset\n",
    "\n",
    "print('The Accuracy  on the training dataset is: ', lm.score(X_train, y_train) )\n",
    "print('The Accuracy n2  on the training dataset is: ',r2_score(y_train,y_pred) )   \n",
    "\n",
    "print(\"\")\n",
    "# Model Accuracy on testing dataset\n",
    "print('The Accuracy  on the testing dataset is: ', lm.score(X_test, y_test) )\n",
    "\n",
    "print(\"\")\n",
    "# The Root Mean Squared Error (RMSE)\n",
    "print('The RMSE  on the training dataset is: ',sqrt(mean_squared_error(y_train,y_pred)))\n",
    "print('The RMSE  on the testing dataset is: ',sqrt(mean_squared_error(y_test,lm.predict(X_test))))\n",
    "\n",
    "print(\"\")\n",
    "# The Mean Absolute Error (MAE)\n",
    "print('The MAE  on the training dataset is: ',mean_absolute_error(y_train,y_pred))\n",
    "print('The MAE  on the testing dataset is: ',mean_absolute_error(y_test,lm.predict(X_test)))\n",
    "\n",
    "\n",
    "print(\"\")\n",
    "# Coefficients\n",
    "print('Coefficients: ', lm.coef_ )\n",
    "\n",
    "print(\"\")\n",
    "# The Intercept\n",
    "print('Intercept: ', lm.intercept_)\n",
    "\n",
    "\n",
    "# R2 Link: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html\n",
    "# RMSE Link: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html\n",
    "# MAE Link: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_absolute_error.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes:\n",
    "    \n",
    "- The accuracy score represents the coefficient of determination ( 𝑅2 ). This is at max 1, but can be negative. It will be 0 if you predict the mean of y for all observations.\n",
    "\n",
    "- The R Square is measure of how close the data are to the fitted regression line. \n",
    "\n",
    "- In this case we can say that our model explains 79% of the training data & 75% of the testing data\n",
    "\n",
    "\n",
    "- The RMSE is the standard deviation of the residuals. Residuals is the difference between the predicted value and the  regression line. Hence RMSE is a measure of how spread your residuals are.\n",
    "\n",
    "- The mean absolute error (MAE) is the average of all the absolute errors. The absolute error is the difference between the true value (y_train) and the predicted value (y_pred).\n",
    "\n",
    "- Coeff are the weights\n",
    "\n",
    "- The intercept is the expected mean value of Y when all X=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Actuals Vs Predicted\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "\n",
    "plt.scatter(y_train, y_pred, c='green')\n",
    "plt.plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], 'k--', c='red', lw=3)\n",
    "plt.xlabel('Actuals')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.title('Actuals Vs Predicted Values')\n",
    "# increase size\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ploting Residuals\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "\n",
    "sns.residplot(x=y_train, y=y_pred, color='green')\n",
    "plt.xlabel('Actual Revenue')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Actuals Vs Residuals')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The next step should be to go back, remove more outliers and check if our model can be improved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. How to use our L. Regression model to Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passing Coeff into a table\n",
    "\n",
    "Coeff = lm.coef_\n",
    "Coeff.shape\n",
    "\n",
    "# Reshaping\n",
    "Coeff = Coeff.reshape(-1,12)\n",
    "\n",
    "\n",
    "# Creating a Dataframe\n",
    "Coeff_df = pd.DataFrame(Coeff, columns = [X2.columns])\n",
    "\n",
    "# Displaying \n",
    "Coeff_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making Predictions                        \n",
    "\n",
    "# Variables\n",
    "\n",
    "Day = 'Day_Name_Friday'\n",
    "Visitors = 8000\n",
    "Marketing_Spend = 5000\n",
    "Promo = 'Promo_Promotion Red'\n",
    "\n",
    "# Regression Formula for pred\n",
    "# y = a + bx + ..   # Where y = price, a = intercept,  b = no. of Visitors and x = coefficient of engine size\n",
    "\n",
    "# Prediction Calculator\n",
    "pred1 = (lm.intercept_) + (Coeff_df[Day].values[0] * 1) + (Coeff_df['Visitors'].values[0] * Visitors) + (Coeff_df['Marketing Spend'].values[0] * Marketing_Spend) + (Coeff_df[Promo].values[0] * 1)\n",
    "\n",
    "print('The predicted Revenue is: ', pred1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################## Where we can apply this as a business and how? #######################################\n",
    "\n",
    "\n",
    "# 1. We can create a prediction tool for upcoming marketing campaigns \n",
    "\n",
    "# 2. We can analyse past marketing campaigns and suggest what work, what did not work, \n",
    "# how much money we could have made and suggest changes - campaign optimisation\n",
    "\n",
    "# 3. We can predict the potential of Stores or Accounts or Channels or Campaigns and compare that with what they actually achieving.\n",
    "# Then we can grade them and make suggestions (over achieving vs underachieving Vs potential Accounts)\n",
    "\n",
    "# 4. We can deploy the model in a server/cloud and automatically adjusting the campaigns that generate the most money depending on the day.\n",
    "# At the same time, keep running A/B testing with new campaigns (ideas) and re-adjust if needed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
